import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# from sklearn.datasets import load_wine
from sklearn.preprocessing import StandardScaler
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.covariance import LedoitWolf
from sklearn.neighbors import KNeighborsClassifier

class RegularizedMahalanobisDistanceClassifier:
    def __init__(self, reg_param=1e-5):
        self.mean_vectors = {}
        self.covariance_matrices = {}
        self.class_labels = []
        self.reg_param = reg_param

    def fit(self, X, y):
        self.class_labels = np.unique(y)
        for label in self.class_labels:
            X_label = X[y == label]
            self.mean_vectors[label] = np.mean(X_label, axis=0)
            self.covariance_matrices[label] = np.cov(X_label.T) + np.eye(X.shape[1]) * self.reg_param

    def predict(self, X):
        y_pred = []
        for x in X:
            distances = []
            for label in self.class_labels:
                mean_vector = self.mean_vectors[label]
                covariance_matrix = self.covariance_matrices[label]
                try:
                    distance = np.sqrt((x-mean_vector).dot(np.linalg.inv(covariance_matrix)).dot((x-mean_vector).T))
                except np.linalg.LinAlgError:
                    # if covariance matrix is singular, use pseudo-inverse instead
                    covariance_matrix += np.eye(covariance_matrix.shape[0]) * self.reg_param
                    distance = np.sqrt((x-mean_vector).dot(np.linalg.pinv(covariance_matrix)).dot((x-mean_vector).T))
                distances.append(distance)
            min_distance_idx = np.argmin(distances)
            y_pred.append(self.class_labels[min_distance_idx])
        return y_pred

from sklearn.datasets import load_digits
digits = load_digits()
X=digits.data
y=digits.target

scaler = StandardScaler()
X_std = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size=0.3, random_state=42)

# 添加噪声
noise = np.random.normal(0, 0.5, size=X_train.shape)
X_train = X_train + noise

n_components = min(X_train.shape[1], len(np.unique(y_train))-1)
accuracy_maha = []
accuracy_knn = []

for n in range(1, n_components+1):
    # Fit a MDC Classifier
    lda = LinearDiscriminantAnalysis(n_components=n)
    X_train_lda = lda.fit_transform(X_train, y_train)

    clf = RegularizedMahalanobisDistanceClassifier()
    clf.fit(X_train_lda, y_train)

    X_test_lda = lda.transform(X_test)
    y_pred = clf.predict(X_test_lda)

    accuracy_maha.append(accuracy_score(y_test, y_pred))


    # Fit a KNN Classifier
    knn = KNeighborsClassifier()
    knn.fit(X_train_lda, y_train)

    y_pred_knn = knn.predict(X_test_lda)

    accuracy_knn.append(accuracy_score(y_test, y_pred_knn))


print(accuracy_maha)
print(accuracy_knn)

plt.plot(range(1, n_components+1), accuracy_maha, label='MDC')
plt.plot(range(1, n_components+1), accuracy_knn, label='KNN')

plt.title('Accuracy vs Number of Components')
plt.xlabel('Number of Components')
plt.ylabel('Accuracy')

# 添加图例
plt.legend()

plt.show()





