import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# from sklearn.datasets import load_wine
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.covariance import LedoitWolf

from sklearn.neighbors import KNeighborsClassifier


class RegularizedMahalanobisDistanceClassifier:
    def __init__(self, reg_param=1e-5):
        self.mean_vectors = {}
        self.covariance_matrices = {}
        self.class_labels = []
        self.reg_param = reg_param

    def fit(self, X, y):
        self.class_labels = np.unique(y)
        for label in self.class_labels:
            X_label = X[y == label]
            self.mean_vectors[label] = np.mean(X_label, axis=0)
            self.covariance_matrices[label] = np.cov(X_label.T) + np.eye(X.shape[1]) * self.reg_param

    def predict(self, X):
        y_pred = []
        for x in X:
            distances = []
            for label in self.class_labels:
                mean_vector = self.mean_vectors[label]
                covariance_matrix = self.covariance_matrices[label]
                try:
                    distance = np.sqrt((x - mean_vector).dot(np.linalg.inv(covariance_matrix)).dot((x - mean_vector).T))
                except np.linalg.LinAlgError:
                    # if covariance matrix is singular, use pseudo-inverse instead
                    covariance_matrix += np.eye(covariance_matrix.shape[0]) * self.reg_param
                    distance = np.sqrt((x - mean_vector).dot(np.linalg.pinv(covariance_matrix)).dot((x - mean_vector).T))
                distances.append(distance)
            min_distance_idx = np.argmin(distances)
            y_pred.append(self.class_labels[min_distance_idx])
        return y_pred

from sklearn.datasets import load_digits
digits = load_digits()
X=digits.data
y=digits.target

# 标准化数据
scaler = StandardScaler()
X_std= scaler.fit_transform(X)

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size=0.3, random_state=42)

# 添加噪声
noise = np.random.normal(0, 0.5, size=X_train.shape)
X_train = X_train + noise

# Perform PCA and transform the data
n_components = np.arange(1, X.shape[1] + 1)
accuracy_maha = []
accuracy_knn = []

for n in n_components:
    pca = PCA(n_components=n)
    X_train_pca = pca.fit_transform(X_train)
    X_test_pca = pca.transform(X_test)

    # Fit a Regularized Mahalanobis Distance Classifier
    clf = RegularizedMahalanobisDistanceClassifier()
    clf.fit(X_train_pca, y_train)

    # Predict the test set
    y_pred = clf.predict(X_test_pca)
    # Calculate the accuracy
    accuracy_maha.append(accuracy_score(y_test, y_pred))


    # Fit a KNN Classifier
    knn = KNeighborsClassifier()
    knn.fit(X_train_pca, y_train)

    y_pred_knn = knn.predict(X_test_pca)

    accuracy_knn.append(accuracy_score(y_test, y_pred_knn))

print(accuracy_maha)
print(accuracy_knn)

# Plot the accuracy
plt.plot(n_components, accuracy_maha, label='MDC')

plt.plot(n_components, accuracy_knn, label='KNN')

plt.title('Accuracy vs Number of Components')
plt.xlabel('Number of Components')
plt.ylabel('Accuracy')

# 添加图例
plt.legend()

plt.show()

